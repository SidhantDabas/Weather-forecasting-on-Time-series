{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection Sample for WRF dataset\n",
    "\n",
    "Demo of a feature selection procedure using PDP, ALE and Random Forest Selection\n",
    "\n",
    "- Target observation variable : AirTemp_01HrMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.models import NLinearModel\n",
    "from darts.utils.missing_values import extract_subseries\n",
    "from darts.dataprocessing.transformers.scaler import Scaler\n",
    "from darts.metrics.metrics import mae, mse, mape\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "data_taiao = '/Dissertation/taiao_data'\n",
    "# ICAO id of Wellington Airport\n",
    "station_icao = 'NZAPA'\n",
    "\n",
    "# WMO id of Wellington Airport\n",
    "station_wmo = '93245'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seting Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'AirTemp_01HrMax'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining all the station data in Observed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = glob.glob(f'{data_taiao}/1minobs/{station_icao}/**/*202*.csv', recursive=True)\n",
    "obs_csv = pd.concat([pd.read_csv(file, index_col='time', parse_dates=True)\n",
    "                     for file in file_paths]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Celsius to Kelvin for all relevant temperature columns\n",
    "obs_csv['AirTemp_01HrMax'] = obs_csv['AirTemp_01HrMax'] + 273.15\n",
    "obs_csv['AirTemp_01HrMin'] = obs_csv['AirTemp_01HrMin'] + 273.15\n",
    "obs_csv['AirTemp_01MnAvg'] = obs_csv['AirTemp_01MnAvg'] + 273.15\n",
    "obs_csv['DewTemp_01MnAvg'] = obs_csv['DewTemp_01MnAvg'] + 273.15\n",
    "obs_csv['EqpTemp_01MnAvg'] = obs_csv['EqpTemp_01MnAvg'] + 273.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WRF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = glob.glob(f'{data_taiao}/nwp/ARWECMWFcld_single_nz4km/{station_wmo}/*202*.nc')\n",
    "wrf_data = xr.open_mfdataset([file for file in file_paths],\n",
    "                             engine='netcdf4',\n",
    "                             concat_dim='run',\n",
    "                             combine='nested',).dropna(dim='run', how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Covariates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns to drop in wrf (more than 40% NaN): \n",
    "- 'ccld2_height', 'ccld2_depth', 'ccld1_height', \n",
    "- 'ccld1_depth', 'mcld_height', 'lcld2_height', \n",
    "- 'hcld_height', 'lcld1_height', 'tcld_height'\n",
    "\n",
    "A few columns were dropped based on the nan values present in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_covariates_wrf = ['ccld1_amount', 'ccld2_amount', 'cldthk', 'fzl', 'hcld_amount', \n",
    "                         'itq', 'itt', 'lapprs850hPa', 'lapprs900hPa', 'laptmk900hPa', \n",
    "                         'lcld1_amount', 'lcld2_amount', 'lwdown', 'mcld_amount', 'mgust', \n",
    "                         'nhclg', 'omg500hPa', 'output_product_version', 'pblh', 'rhu500hPa', \n",
    "                         'rhu700hPa', 'rhu850hPa', 'rtb', 'rtc', 'rte', 'scape', 'sfp', \n",
    "                         'stb500hPa', 'stb500m', 'stb850hPa', 'swdown', 't2mc', 'tcld_amount', \n",
    "                         'tcldr', 'tdp10m', 'tdp700hPa', 'thw700hPa', 'thw850hPa', 'thw900hPa', \n",
    "                         'tmc10m', 'tmc500m', 'tmc700hPa', 'tmc850m', 'uuu10m', 'uuu115m', 'uuu30m', \n",
    "                         'uuu45m', 'uuu500hPa', 'uuu500m', 'uuu60m', 'uuu700hPa', 'uuu75m', \n",
    "                         'uuu850hPa', 'uuu90m', 'vvv10m', 'vvv115m', 'vvv30m', 'vvv45m', 'vvv500hPa', \n",
    "                         'vvv500m', 'vvv60m', 'vvv700hPa', 'vvv75m', 'vvv850hPa', 'vvv90m', 'wdr500m', \n",
    "                         'wdr700hPa', 'wdr850hPa', 'wspk10m', 'wspk115m', 'wspk200m', 'wspk30m', \n",
    "                         'wspk45m', 'wspk500hPa', 'wspk500m', 'wspk60m', 'wspk75m', 'wspk850hPa', \n",
    "                         'wspk90m', 'www500hPa', 'www700hPa', 'www900hPa']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge obs and NWP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the valid_time of the NWP forecast, to join with the observation time\n",
    "wrf_data_df = (wrf_data.to_dataframe().reset_index()\n",
    "          .assign(valid_time = lambda x : x.run + x.prognosis_period).set_index('valid_time'))\n",
    "wrf_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns in Celsius that need to be converted to Kelvin\n",
    "celsius_columns = ['TMaxDaily', 'TMinDaily','t2_24h_max','t2_24h_min','t2m', 't850p', 'tdwpt', 'tmax', 'tmin']\n",
    "\n",
    "# Convert each column from Celsius to Kelvin by adding 273.15\n",
    "for col in celsius_columns:\n",
    "    if col in wrf_data_df.columns:  # Ensure the column exists in the DataFrame\n",
    "        wrf_data_df[col] = wrf_data_df[col] + 273.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = obs_csv[[target]].join(wrf_data_df[['run', 'prognosis_period'] + future_covariates_wrf], how='inner').dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduped_df = joined_df.reset_index().sort_values(['time', 'run']).drop_duplicates(subset='time', keep='last').set_index('time')\n",
    "deduped_df = deduped_df.drop(['run', 'prognosis_period'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = deduped_df.loc[deduped_df.index.year < 2022]\n",
    "valid_df = deduped_df.loc[deduped_df.index.year == 2022]\n",
    "test_df = deduped_df.loc[deduped_df.index.year == 2023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try to forecast 12h ahead based on the previous 12h\n",
    "input_chunk_length=2\n",
    "output_chunk_length=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from darts.metrics import mae, mse, mape\n",
    "\n",
    "# Test metrics with last_points_only=True\n",
    "metrics = model.backtest(\n",
    "    series=test_target,\n",
    "    future_covariates=test_future_cov,\n",
    "    forecast_horizon=output_chunk_length,\n",
    "    metric=[mae, mse, mape],\n",
    "    last_points_only=True,  # Only consider the last point of each forecast\n",
    "    retrain=False\n",
    ")\n",
    "\n",
    "# Calculate the average metrics across all iterations\n",
    "average_metrics = np.mean(metrics, axis=0)\n",
    "print(\"Average MAE, MSE, MAPE:\", average_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Using Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train = train_df.drop(columns=[target])\n",
    "X_valid = valid_df.drop(columns=[target])\n",
    "y_train = train_df[target]\n",
    "y_valid = valid_df[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "from PyALE import ale\n",
    "\n",
    "# Train the RandomForestRegressor model\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on validation set\n",
    "accuracy_before = rf.score(X_valid, y_valid)\n",
    "print(f'Accuracy before feature selection: {accuracy_before:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting feature importances and ranking them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importances\n",
    "importances = rf.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "feature_importance_df = pd.DataFrame({'Feature_RF': feature_names, 'Importance_RF': importances})\n",
    "\n",
    "# Rank features by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance_RF', ascending=False)\n",
    "feature_importance_df=feature_importance_df.head(20)\n",
    "\n",
    "top_features = feature_importance_df['Feature_RF'][:20].values\n",
    "X_train_selected = X_train[top_features]\n",
    "X_valid_selected = X_valid[top_features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-train the model with the selected top features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-train the model with the selected top features\n",
    "rf2 = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf2.fit(X_train_selected, y_train)  # Ensure to use the selected features here\n",
    "\n",
    "# Evaluate the model on validation set\n",
    "accuracy_after = rf2.score(X_valid_selected, y_valid)\n",
    "print(f'Accuracy after feature selection: {accuracy_after:.3f}')\n",
    "y_predRF = rf2.predict(X_valid_selected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALE PLOT FOR Randomforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Number of features\n",
    "num_features = len(top_features)\n",
    "\n",
    "# Set up a grid of subplots (e.g., 4 rows and 5 columns for 20 features)\n",
    "rows, cols = 4, 5\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, 10))\n",
    "axes = axes.flatten()  # Flatten the 2D array of axes to 1D for easier indexing\n",
    "\n",
    "# Dictionary to store ALE importance values\n",
    "ale_forest_importance = {}\n",
    "\n",
    "# Iterate through each feature and plot ALE\n",
    "for idx, feature in enumerate(top_features):\n",
    "    ale_eff = ale(\n",
    "        X=X_train_selected,  # The test data as a pandas DataFrame\n",
    "        model=rf2,  # The model\n",
    "        feature=[feature],  # Feature for which ALE is computed\n",
    "        grid_size=10,  # Adjust the grid size as needed\n",
    "        C=0.95  # 95% confidence interval\n",
    "    )\n",
    "    \n",
    "    # Get the ALE effect values\n",
    "    ale_effects = ale_eff['eff']\n",
    "    feature_values = ale_eff.index  \n",
    "    # Plot the ALE effect in the appropriate subplot\n",
    "    axes[idx].plot(feature_values, ale_effects, label=f'ALE of {feature}')\n",
    "    axes[idx].set_title(f'ALE for {feature}')\n",
    "    axes[idx].set_xlabel(feature)\n",
    "    axes[idx].set_ylabel('ALE Effect')\n",
    "    \n",
    "    # Optional: Customize appearance (like grid, labels, etc.)\n",
    "    axes[idx].grid(True)\n",
    "    \n",
    "    # Calculate the mean absolute ALE effect for the feature\n",
    "    mean_abs_ale = np.mean(np.abs(ale_effects))\n",
    "    ale_forest_importance[feature] = mean_abs_ale\n",
    "\n",
    "# Remove any unused subplots\n",
    "for ax in axes[num_features:]:\n",
    "    fig.delaxes(ax)\n",
    "\n",
    "# Adjust layout so the plots don't overlap\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show the figure (optional)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the feature importance to a DataFrame and sort by importance\n",
    "ale_forest_importance_df = pd.DataFrame(list(ale_forest_importance.items()), columns=['Feature_AleRF', 'Importance_AleRF'])\n",
    "ale_forest_importance_df = ale_forest_importance_df.sort_values(by='Importance_AleRF', ascending=False)\n",
    "csv_df= pd.concat([feature_importance_df.reset_index(drop=True),ale_forest_importance_df.reset_index(drop=True)],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training MLP with StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "X_valid_scaled = scaler.transform(X_valid_selected)\n",
    "\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train_selected.columns)\n",
    "# Initialize the MLP Regressor\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(120),  # You can adjust these values\n",
    "                   activation='tanh', \n",
    "                   solver='sgd',\n",
    "                   max_iter=1000, \n",
    "                   random_state=42\n",
    "                   )\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred = mlp.predict(X_valid_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALE PLOT FOR MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a grid of subplots (e.g., 4 rows and 5 columns for 20 features)\n",
    "rows, cols = 4, 5\n",
    "fig, axes_mlp = plt.subplots(rows, cols, figsize=(15, 10))\n",
    "axes_mlp = axes_mlp.flatten()  # Flatten the 2D array of axes to 1D for easier indexing\n",
    "mlp_importance = {}\n",
    "# Iterate through each feature and calculate ALE\n",
    "for idx, feature in enumerate(top_features):\n",
    "    print(f\"Calculating ALE for {feature}...\")\n",
    "    \n",
    "    # Calculate the ALE for the current feature\n",
    "    ale_eff = ale(\n",
    "        X=X_train_scaled_df,           # The scaled training data\n",
    "        model=mlp,              # The trained MLP Regressor model\n",
    "        feature=[feature],      # The current feature to analyze\n",
    "        grid_size=10,           # You can adjust the grid size as needed\n",
    "        C=0.95                  # 95% confidence interval\n",
    "    )\n",
    "    \n",
    "    # Get the ALE effect values (typically stored in the 'eff' column of the result)\n",
    "    ale_effects = ale_eff['eff']\n",
    "    feature_values = ale_eff.index  \n",
    "    # Plot the ALE effect in the appropriate subplot\n",
    "    axes_mlp[idx].plot(feature_values, ale_effects, label=f'ALE of {feature}')\n",
    "    axes_mlp[idx].set_title(f'ALE for {feature}')\n",
    "    axes_mlp[idx].set_xlabel(feature)\n",
    "    axes_mlp[idx].set_ylabel('ALE Effect')\n",
    "    axes_mlp[idx].grid(True)\n",
    "    # Calculate the mean absolute ALE effect for the feature\n",
    "    mean_abs_ale = np.mean(np.abs(ale_effects))\n",
    "    \n",
    "    # Store the feature and its importance score\n",
    "    mlp_importance[feature] = mean_abs_ale\n",
    "# Remove any unused subplots\n",
    "for ax in axes_mlp[num_features:]:\n",
    "    fig.delaxes(ax)\n",
    "\n",
    "# Adjust layout so the plots don't overlap\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show the figure (optional)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the feature importance dictionary to a DataFrame and sort it\n",
    "mlp_ale_importance_df = pd.DataFrame(list(mlp_importance.items()), columns=['Feature_Ale_MLP', 'Importance_Ale_MLP'])\n",
    "mlp_ale_importance_df = mlp_ale_importance_df.sort_values(by='Importance_Ale_MLP', ascending=False)\n",
    "mlp_ale_importance_df['Importance_Ale_MLP'] = mlp_ale_importance_df['Importance_Ale_MLP'].apply(lambda x: f'{x:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df= pd.concat([mlp_ale_importance_df.reset_index(drop=True),csv_df.reset_index(drop=True)],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDP plot for mlp and storing the ranking(stadard deviation wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import partial_dependence\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Number of columns in the grid\n",
    "n_cols = 3  \n",
    "n_rows = (len(top_features) + n_cols - 1) // n_cols  # Calculate rows based on the number of features and columns\n",
    "\n",
    "# Create a figure with subplots (adjust size accordingly)\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))  \n",
    "axes = axes.flatten()  # Flatten axes to iterate easily\n",
    "\n",
    "# Create a dictionary to store the PDP importance scores\n",
    "mlp_pdp_stddev = {}\n",
    "\n",
    "# Iterate through each feature and calculate PDP\n",
    "for idx, feature in enumerate(top_features):  \n",
    "    print(f\"Calculating PDP for {feature}...\")\n",
    "    \n",
    "    # Get the feature index in X_train_selected (since PDP works on indices, not names)\n",
    "    feature_idx = X_train_scaled_df.columns.get_loc(feature)\n",
    "    \n",
    "    # Calculate the PDP for the current feature and plot it on the respective subplot\n",
    "    display = PartialDependenceDisplay.from_estimator(\n",
    "        mlp, \n",
    "        X=X_train_scaled, \n",
    "        features=[feature_idx], \n",
    "        ax=axes[idx],  # Use the appropriate subplot\n",
    "        grid_resolution=100\n",
    "    )\n",
    "    \n",
    "    # Set the title for each subplot\n",
    "    axes[idx].set_title(f\"PDP for {feature}\")\n",
    "    \n",
    "    # Extract the partial dependence values from the display object\n",
    "    pdp_effects = display.pd_results[0]['average'][0]\n",
    "    \n",
    "    # Calculate the standard deviation of PDP effects for the feature\n",
    "    stddev_pdp = np.std(pdp_effects)\n",
    "    \n",
    "    # Store the feature and its standard deviation score\n",
    "    mlp_pdp_stddev[feature] = stddev_pdp\n",
    "\n",
    "# Hide unused subplots if there are any\n",
    "for i in range(len(top_features), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure with all PDP plots\n",
    "plt.savefig('pdp_MLP_plots.png', dpi=300)\n",
    "\n",
    "# Show the combined plot\n",
    "plt.show()\n",
    "\n",
    "# Convert the PDP standard deviation dictionary to a DataFrame and sort it\n",
    "mlp_pdp_stddev_df = pd.DataFrame(list(mlp_pdp_stddev.items()), columns=['Feature_PDP_MLP', 'StdDev_PDP_MLP'])\n",
    "mlp_pdp_stddev_df = mlp_pdp_stddev_df.sort_values(by='StdDev_PDP_MLP', ascending=False)\n",
    "mlp_pdp_stddev_df['StdDev_PDP_MLP'] = mlp_pdp_stddev_df['StdDev_PDP_MLP'].apply(lambda x: f'{x:.6f}')\n",
    "\n",
    "# Display the ranked features based on PDP standard deviation\n",
    "print(\"Ranked feature importance based on PDP Standard Deviation:\")\n",
    "print(mlp_pdp_stddev_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df= pd.concat([mlp_pdp_stddev_df.reset_index(drop=True),csv_df.reset_index(drop=True)],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDP plot for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import partial_dependence\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Number of columns in the grid\n",
    "n_cols = 3  \n",
    "n_rows = (len(top_features) + n_cols - 1) // n_cols  # Calculate rows based on the number of features and columns\n",
    "\n",
    "# Create a figure with subplots (adjust size accordingly)\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))  \n",
    "axes = axes.flatten()  # Flatten axes to iterate easily\n",
    "\n",
    "# Create a dictionary to store the PDP standard deviation scores\n",
    "rf_pdp_stddev = {}\n",
    "\n",
    "# Iterate through each feature and calculate PDP\n",
    "for idx, feature in enumerate(top_features):  \n",
    "    print(f\"Calculating PDP for {feature}...\")\n",
    "    \n",
    "    # Get the feature index in X_train_selected (since PDP works on indices, not names)\n",
    "    feature_idx = X_train_scaled_df.columns.get_loc(feature)\n",
    "    \n",
    "    # Calculate the PDP for the current feature and plot it on the respective subplot\n",
    "    display = PartialDependenceDisplay.from_estimator(\n",
    "        rf2, \n",
    "        X=X_train_selected, \n",
    "        features=[feature_idx], \n",
    "        ax=axes[idx],  # Use the appropriate subplot\n",
    "        grid_resolution=100\n",
    "    )\n",
    "\n",
    "    # Set the title for each subplot\n",
    "    axes[idx].set_title(f\"PDP for {feature}\")\n",
    "\n",
    "    # Extract the partial dependence values from the display object\n",
    "    pdp_effects = display.pd_results[0]['average'][0]\n",
    "    \n",
    "    # Calculate the standard deviation of PDP effects for the feature\n",
    "    stddev_pdp = np.std(pdp_effects)\n",
    "    \n",
    "    # Store the feature and its standard deviation score\n",
    "    rf_pdp_stddev[feature] = stddev_pdp\n",
    "\n",
    "# Hide unused subplots if there are any\n",
    "for i in range(len(top_features), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure with all PDP plots\n",
    "plt.savefig('pdp_RF_plots.png', dpi=300)\n",
    "\n",
    "# Show the combined plot\n",
    "plt.show()\n",
    "\n",
    "# Convert the PDP standard deviation dictionary to a DataFrame and sort it\n",
    "rf_pdp_stddev_df = pd.DataFrame(list(rf_pdp_stddev.items()), columns=['Feature_PDP_RF', 'StdDev_PDP_RF'])\n",
    "rf_pdp_stddev_df = rf_pdp_stddev_df.sort_values(by='StdDev_PDP_RF', ascending=False)\n",
    "rf_pdp_stddev_df['StdDev_PDP_RF'] = rf_pdp_stddev_df['StdDev_PDP_RF'].apply(lambda x: f'{x:.6f}')\n",
    "\n",
    "# Display the ranked features based on PDP standard deviation\n",
    "print(\"Ranked feature importance based on PDP Standard Deviation:\")\n",
    "print(rf_pdp_stddev_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df= pd.concat([rf_pdp_stddev_df.reset_index(drop=True),csv_df.reset_index(drop=True)],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating MLP and RFs' MAE, MSE and R2 error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate regression metrics for Random Forest\n",
    "mse_rf = mean_squared_error(y_valid, y_predRF)\n",
    "mae_rf = mean_absolute_error(y_valid, y_predRF)\n",
    "r2_rf = r2_score(y_valid, y_predRF)\n",
    "print(f'Model: RF')\n",
    "print(f'Mean Squared Error (MSE): {mse_rf:.4f}')\n",
    "print(f'Mean Absolute Error (MAE): {mae_rf:.4f}')\n",
    "print(f'R-squared (R2): {r2_rf:.4f}')\n",
    "\n",
    "# Calculate regression metrics for MLP\n",
    "mse_mlp = mean_squared_error(y_valid, y_pred)\n",
    "mae_mlp = mean_absolute_error(y_valid, y_pred)\n",
    "r2_mlp = r2_score(y_valid, y_pred)\n",
    "print(f'Model: MLP')\n",
    "print(f'Mean Squared Error (MSE): {mse_mlp:.4f}')\n",
    "print(f'Mean Absolute Error (MAE): {mae_mlp:.4f}')\n",
    "print(f'R-squared (R2): {r2_mlp:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store metrics in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metrics for both RF and MLP\n",
    "metrics_data = {\n",
    "    'Model': ['RF', 'MLP'],\n",
    "    'MSE': [mse_rf, mse_mlp],  # mse_rf and mse_mlp from the respective models\n",
    "    'MAE': [mae_rf, mae_mlp],  # mae_rf and mae_mlp from the respective models\n",
    "    'R2': [r2_rf, r2_mlp]      # r2_rf and r2_mlp from the respective models\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Concatenate the metrics to the feature importance data\n",
    "# Reset index to avoid index alignment issues\n",
    "csv_df = pd.concat([csv_df.reset_index(drop=True), metrics_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Save the updated DataFrame to a CSV file\n",
    "csv_df.to_csv('Feature_ECMWF_Ensemble_with_metrics.csv', index=False)\n",
    "\n",
    "print(\"Metrics and feature importances saved to 'Feature_ECMWF_Ensemble_with_metrics.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlpp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
