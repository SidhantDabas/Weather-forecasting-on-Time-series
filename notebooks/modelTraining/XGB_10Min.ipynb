{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-FRwULbJH7h"
      },
      "source": [
        "# Ten minute dataset XGBOOST\n",
        "Importing and setting base directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1ynK1IpJH7k",
        "outputId": "55a4e35f-6f38-46ea-e270-040c3ffa303c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "from darts import TimeSeries\n",
        "from darts.models import NLinearModel\n",
        "from darts.utils.missing_values import extract_subseries\n",
        "from darts.dataprocessing.transformers.scaler import Scaler\n",
        "from darts.metrics.metrics import mae, mse, mape\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Define station information\n",
        "stations = {\n",
        "    '93439': {'icao': 'NZWNA', 'name': 'Wellington Aero'},\n",
        "    '93110': {'icao': 'NZAAA', 'name': 'Auckland Aero'},\n",
        "    '93831': {'icao': 'NZQNA', 'name': 'Queenstown Aero'},\n",
        "    '93781': {'icao': 'NZCHA', 'name': 'Christchurch Aero'},\n",
        "    '93245': {'icao': 'NZAPA', 'name': 'Taupo Airport Aws'}\n",
        "}\n",
        "\n",
        "# List to store DataFrames for each station\n",
        "station_dfs = []\n",
        "\n",
        "# Directory where the CSV files are stored\n",
        "base_dir = r\"C:\\Users\\sidha\\Desktop\\Final Dissertatoin\\10min\\ten_min.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kd3tBdpsJH7n"
      },
      "source": [
        "## Reading CSV files for different stations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toGXZVrlJH7o"
      },
      "outputs": [],
      "source": [
        "# Concatenate all station DataFrames along rows, keeping the time and station_id as a multi-index\n",
        "combined_df = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHtls8rGJH7o"
      },
      "source": [
        "## Setting covariates and target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zu-qNH8xJH7p"
      },
      "outputs": [],
      "source": [
        "# Reset the multi-index and set `time` as the index\n",
        "combined_df = combined_df.reset_index().set_index('time').dropna()\n",
        "\n",
        "# Define covariate categories\n",
        "future_covariates_wrf = [\n",
        "    'wspk10m', 'uuu30m', 'vvv10m', 'lapprs900hPa', 'uuu10m',\n",
        "    'wspk115m', 'vvv850hPa', 'wspk850hPa', 'mgust', 'vvv30m',\n",
        "    'swdown', 'uuu500m'\n",
        "]\n",
        "future_covariates_ecmwf_ensemble = [\n",
        "    'v100m', 'tdwpt', 'f10m', 'g10m', 'gust10_inst', 'f100m',\n",
        "    'tmax', 'u100m', 'u10m', 'v10m'\n",
        "]\n",
        "future_covariates_ecmwf_single = [\n",
        "    'u100m', 'f100m', 'rh', 'ws850p', 'g10m', 'u10m',\n",
        "    'u925p', 'u850p', 'f10m', 'v925p'\n",
        "]\n",
        "past_covariates = [\n",
        "    'Data_Time_Day__', 'Data_Time_Hour_',\n",
        "    'Data_Time_Month', 'Data_Time_Year_'\n",
        "]\n",
        "static_covariates = ['station_id']\n",
        "\n",
        "# Add suffixes to future covariates\n",
        "future_covariates_ecmwf_ensemble = [col + '_EE' for col in future_covariates_ecmwf_ensemble]\n",
        "future_covariates_ecmwf_single = [col + '_ES' for col in future_covariates_ecmwf_single]\n",
        "\n",
        "# Combine all future covariates\n",
        "future_covariates = future_covariates_wrf + future_covariates_ecmwf_ensemble + future_covariates_ecmwf_single\n",
        "\n",
        "# Define target variable\n",
        "target = 'WindSpd_10MnAvg'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m53qIxHHJH7p"
      },
      "source": [
        "## Creating a timeseries object grouped by station id."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9-f0s-yJH7p"
      },
      "outputs": [],
      "source": [
        "# Ensure `time` is the index and reset it for processing\n",
        "combined_df = combined_df.reset_index()\n",
        "\n",
        "# Create TimeSeries objects grouped by `station_id`, embedding static covariates\n",
        "series_list = TimeSeries.from_group_dataframe(\n",
        "    combined_df,\n",
        "    time_col=\"time\",\n",
        "    group_cols=\"station_id\",  # Group by station\n",
        "    static_cols=[\"Stn_Numeric_ID_\"],  # Static covariates (if any)\n",
        "    value_cols=[target] + past_covariates + future_covariates,  # Time-varying columns\n",
        "    fill_missing_dates=True,  # Fill missing dates to ensure consistent frequency\n",
        "    freq=\"10T\",  # Specify the frequency explicitly (10-minute intervals in this case)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ot7Og5sJH7q"
      },
      "outputs": [],
      "source": [
        "from darts.dataprocessing.transformers.static_covariates_transformer import StaticCovariatesTransformer\n",
        "\n",
        "# Initialize and apply transformer\n",
        "static_cov_transformer = StaticCovariatesTransformer()\n",
        "series_list_transformed = static_cov_transformer.fit_transform(series_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbtgCXk7KRFQ"
      },
      "source": [
        "### Splitting Data into 20/20/60 ratio(test/valid/train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xg31wj-nJH7q",
        "outputId": "068e2036-9d70-4020-cdf5-1d014b40d6c6"
      },
      "outputs": [],
      "source": [
        "# Define train, validation, and test split ratios\n",
        "train_ratio = 0.6\n",
        "valid_ratio = 0.2\n",
        "\n",
        "# Split the data into train, validation, and test sets\n",
        "stations_splits = {}\n",
        "for ts in series_list_transformed:\n",
        "    station_id = ts.static_covariates['station_id'].iloc[0]  # Get the station ID from static covariates\n",
        "    n_total = len(ts)\n",
        "    n_train = int(train_ratio * n_total)\n",
        "    n_valid = int(valid_ratio * n_total)\n",
        "\n",
        "    # Perform the splits\n",
        "    train_ts = ts[:n_train]\n",
        "    valid_ts = ts[n_train:n_train + n_valid]\n",
        "    test_ts = ts[n_train + n_valid:]\n",
        "\n",
        "    # Store the splits\n",
        "    stations_splits[station_id] = {'train': train_ts, 'valid': valid_ts, 'test': test_ts}\n",
        "\n",
        "    # Verification\n",
        "    total_length = len(train_ts) + len(valid_ts) + len(test_ts)\n",
        "    assert total_length == n_total, f\"Length mismatch for station {station_id}\"\n",
        "    assert len(set(train_ts.time_index).intersection(valid_ts.time_index)) == 0, f\"Overlap between train and valid for station {station_id}\"\n",
        "    assert len(set(valid_ts.time_index).intersection(test_ts.time_index)) == 0, f\"Overlap between valid and test for station {station_id}\"\n",
        "    assert len(set(train_ts.time_index).intersection(test_ts.time_index)) == 0, f\"Overlap between train and test for station {station_id}\"\n",
        "    assert train_ts.static_covariates['station_id'].iloc[0] == station_id, f\"Static covariate mismatch in train set for station {station_id}\"\n",
        "    assert valid_ts.static_covariates['station_id'].iloc[0] == station_id, f\"Static covariate mismatch in valid set for station {station_id}\"\n",
        "    assert test_ts.static_covariates['station_id'].iloc[0] == station_id, f\"Static covariate mismatch in test set for station {station_id}\"\n",
        "\n",
        "print(\"All checks passed successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V74ADIKQJH7r",
        "outputId": "1c193014-e6fb-48f2-d42a-ea9d4238185c"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "train_ts[target].plot()\n",
        "valid_ts[target].plot()\n",
        "test_ts[target].plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zccuQQiLJH7r"
      },
      "outputs": [],
      "source": [
        "# Extraction of long enough time series for Darts\n",
        "def extract_complete_subseries(ts, target, past_covariates, future_covariates, min_timesteps):\n",
        "    \"\"\"\n",
        "    Extract subseries with a minimum number of timesteps.\n",
        "    \"\"\"\n",
        "    # Extract subseries that are long enough\n",
        "    extracted_target = [subserie[target] for subserie in extract_subseries(ts)\n",
        "        if subserie.n_timesteps >= min_timesteps]\n",
        "\n",
        "    # Extract corresponding past covariates\n",
        "    extracted_past_cov = [subserie[past_covariates] for subserie in extract_subseries(ts)\n",
        "        if subserie.n_timesteps >= min_timesteps]\n",
        "\n",
        "    # Extract corresponding future covariates\n",
        "    extracted_future_cov = [subserie[future_covariates] for subserie in extract_subseries(ts)\n",
        "        if subserie.n_timesteps >= min_timesteps]\n",
        "\n",
        "    return extracted_target, extracted_past_cov, extracted_future_cov\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "928HT6H8JH7s"
      },
      "outputs": [],
      "source": [
        "# Function to extract station-specific subsets\n",
        "def extract_station_subsets(splits, target, past_covariates, future_covariates, min_timesteps):\n",
        "    \"\"\"\n",
        "    Extract train, validation, and test subsets for a specific station.\n",
        "    \"\"\"\n",
        "    train_target, train_past_cov, train_future_cov = extract_complete_subseries(\n",
        "        splits['train'], target, past_covariates, future_covariates, min_timesteps\n",
        "    )\n",
        "    valid_target, valid_past_cov, valid_future_cov = extract_complete_subseries(\n",
        "        splits['valid'], target, past_covariates, future_covariates, min_timesteps\n",
        "    )\n",
        "    test_target, test_past_cov, test_future_cov = extract_complete_subseries(\n",
        "        splits['test'], target, past_covariates, future_covariates, min_timesteps\n",
        "    )\n",
        "    return (\n",
        "        train_target, train_past_cov, train_future_cov,\n",
        "        valid_target, valid_past_cov, valid_future_cov,\n",
        "        test_target, test_past_cov, test_future_cov\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pihqClC7JH7s"
      },
      "source": [
        "## Loop to fit and backtest stations for different output chunk lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JheYVnR9JH7s",
        "outputId": "fd2728dd-9681-4e6f-b5a2-fa9f7f069ee5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from darts.models import XGBModel, NLinearModel\n",
        "from darts.metrics import mae, mse, r2_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "# Define the ranges for input and output chunk lengths\n",
        "input_chunk_length = 512\n",
        "output_chunk_lengths = range(18,19)  # Use range(6) for consistency\n",
        "\n",
        "# Initialize a list to store metrics for both models\n",
        "metrics_list = []\n",
        "\n",
        "# Train and evaluate models for each output chunk length\n",
        "for output_chunk_length in output_chunk_lengths:\n",
        "    print(f\"Training with input_chunk_length={input_chunk_length} and output_chunk_length={output_chunk_length}\")\n",
        "\n",
        "    for model_type in [\"XGBoost\"]:  # Iterate over model types\n",
        "        print(f\"Using model type: {model_type}\")\n",
        "\n",
        "        # Initialize a dictionary to store models for each station\n",
        "        models = {}\n",
        "\n",
        "        for station_id, splits in stations_splits.items():\n",
        "            print(f\"  Training for station: {station_id}\")\n",
        "\n",
        "            # Extract subsets specific to this station\n",
        "            (\n",
        "                train_target, train_past_cov, train_future_cov,\n",
        "                valid_target, valid_past_cov, valid_future_cov,\n",
        "                test_target, test_past_cov, test_future_cov\n",
        "            ) = extract_station_subsets(\n",
        "                splits, target, past_covariates, future_covariates,\n",
        "                min_timesteps=input_chunk_length + output_chunk_length\n",
        "            )\n",
        "\n",
        "            # Initialize the model based on type\n",
        "            if model_type == \"XGBoost\":\n",
        "                model = XGBModel(\n",
        "                    lags=list(range(-input_chunk_length, 0)),  # Match input chunk\n",
        "                    lags_past_covariates=list(range(-input_chunk_length, 0)),\n",
        "                    lags_future_covariates=list(range(0, output_chunk_length)),  # Future covariates\n",
        "                    output_chunk_length=output_chunk_length,\n",
        "                    use_static_covariates=True,\n",
        "                    random_state=42\n",
        "                )\n",
        "            elif model_type == \"NLinear\":\n",
        "                model = NLinearModel(\n",
        "                    input_chunk_length=input_chunk_length,\n",
        "                    output_chunk_length=output_chunk_length,\n",
        "                    random_state=42\n",
        "                )\n",
        "\n",
        "            # Measure the start time\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Fit the model using station-specific data\n",
        "            model.fit(\n",
        "                series=train_target,\n",
        "                past_covariates=train_past_cov,\n",
        "                future_covariates=train_future_cov,\n",
        "                val_series=valid_target,\n",
        "                val_past_covariates=valid_past_cov,\n",
        "                val_future_covariates=valid_future_cov\n",
        "            )\n",
        "\n",
        "            models[station_id] = model\n",
        "\n",
        "            # Evaluate using backtesting\n",
        "            metrics = model.backtest(\n",
        "                series=test_target,\n",
        "                past_covariates=test_past_cov,\n",
        "                future_covariates=test_future_cov,\n",
        "                forecast_horizon=output_chunk_length,\n",
        "                metric=[mae, mse, r2_score],\n",
        "                last_points_only=True,\n",
        "                retrain=False\n",
        "            )\n",
        "\n",
        "            # Calculate average metrics\n",
        "            average_metrics = np.mean(metrics, axis=0)\n",
        "            print(f\"    Station {station_id} - Average Metrics (MAE, MSE, R2): {average_metrics}\")\n",
        "            # Measure the end time and calculate the duration\n",
        "            end_time = time.time()\n",
        "            training_time = end_time - start_time\n",
        "            print(f\"    Training time for station {station_id}: {training_time:.2f} seconds\")\n",
        "            # Append metrics for this station, configuration, and model type to the list\n",
        "            metrics_list.append({\n",
        "                \"Station ID\": station_id,\n",
        "                \"Model Type\": model_type,\n",
        "                \"Input Chunk Length\": input_chunk_length,\n",
        "                \"Output Chunk Length\": output_chunk_length,\n",
        "                \"MAE\": average_metrics[0],\n",
        "                \"MSE\": average_metrics[1],\n",
        "                \"R2 Score\": average_metrics[2],\n",
        "                \"Training Time (s)\": training_time\n",
        "            })\n",
        "\n",
        "# Convert metrics list to a DataFrame\n",
        "metrics_df = pd.DataFrame(metrics_list)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "metrics_df.to_csv(\"metrics_combined_xgboost_nlinear_10min.csv\", index=False)\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "print(metrics_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOehJwbRJH7t"
      },
      "source": [
        "## Plot and save metrics to csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38pe-7aYJH7t",
        "outputId": "d870cbb1-7f9e-4534-af02-e187b512ef6c"
      },
      "outputs": [],
      "source": [
        "# Ensure all station IDs are mapped\n",
        "scaled_to_original_mapping = {\n",
        "    0.000000: 93439,\n",
        "    2.0: 93831,\n",
        "    3.0: 93110,\n",
        "    4.0: 93245,\n",
        "    1.000000: 93781,\n",
        "}\n",
        "\n",
        "# Replace scaled Station IDs with original numbers in the DataFrame\n",
        "metrics_xgboost_df[\"Station ID\"] = metrics_xgboost_df[\"Station ID\"].map(scaled_to_original_mapping)\n",
        "\n",
        "# Drop rows with unmapped Station IDs\n",
        "metrics_xgboost_df = metrics_xgboost_df.dropna(subset=[\"Station ID\"])\n",
        "\n",
        "# Convert Station IDs to integers\n",
        "metrics_xgboost_df[\"Station ID\"] = metrics_xgboost_df[\"Station ID\"].astype(int)\n",
        "\n",
        "# Plot MAE vs. Output Chunk Length for each station\n",
        "for station_id in metrics_xgboost_df[\"Station ID\"].unique():\n",
        "    station_data = metrics_xgboost_df[metrics_xgboost_df[\"Station ID\"] == station_id]\n",
        "    plt.plot(\n",
        "        station_data[\"Output Chunk Length\"],\n",
        "        station_data[\"MAE\"],\n",
        "        label=f\"Station {station_id}\"\n",
        "    )\n",
        "\n",
        "# Customize the plot\n",
        "plt.xlabel(\"Output Chunk Length\")\n",
        "plt.ylabel(\"MAE\")\n",
        "plt.title(\"MAE vs Output Chunk Length for All Stations\")\n",
        "plt.legend(title=\"Station\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Save the updated DataFrame to a CSV\n",
        "metrics_xgboost_df.to_csv(\"metrics_xgboost_10_min.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "weather_forecasting",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
